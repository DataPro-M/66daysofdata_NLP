{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3440c614-492a-48b1-8f11-38f8f25a846c",
   "metadata": {},
   "source": [
    "# NLTK: Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0eee62-0f9b-4c33-9007-000d97a2633b",
   "metadata": {},
   "source": [
    "* ref: \n",
    "    - [https://pythonprogramming.net](https://pythonprogramming.net)\n",
    "    - [www.geeksforgeeks.org](https://www.geeksforgeeks.org/python-stemming-words-with-nltk/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e66ce-a34a-4053-82e1-06ec3b9bdef7",
   "metadata": {},
   "source": [
    "* The __NLTK module__ is a massive tool kit, aimed at helping you with the entire Natural Language Processing (NLP) methodology. \n",
    "NLTK will aid you with everything\n",
    "    - from splitting sentences from paragraphs, \n",
    "    - splitting up words, \n",
    "    - recognizing the part of speech of those words, \n",
    "    - highlighting the main subjects, \n",
    "    - and then even with helping your machine to understand what the text is all about. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b0987-5276-4c68-98cd-6c5513dddff5",
   "metadata": {},
   "source": [
    "* quick vocabulary:\n",
    "    \n",
    "    - __Corpus__ - Body of text, singular. __Corpora__ is the plural of this. Example: *A collection of medical journals*.\n",
    "    - __Lexicon__ - Words and their meanings. *Example: English dictionary*. Consider, however, that various fields will have different lexicons. For example: *To a financial investor, the first meaning for the word \"Bull\" is someone who is confident about the market, as compared to the common English lexicon, where the first meaning for the word \"Bull\" is an animal*. As such, there is a special lexicon for financial investors, doctors, children, mechanics, and so on.\n",
    "    - __Token__ - Each \"entity\" that is a part of whatever was split up based on rules. For examples, *each word is a token when a sentence is \"tokenized\" into words*. Each sentence can also be a token, if you tokenized the sentences out of a paragraph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "366f6c51-9c81-4f85-8922-bf5bcf921af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/pi/anaconda3/envs/p36/lib/python3.6/site-packages (3.6.2)\n",
      "Requirement already satisfied: regex in /home/pi/anaconda3/envs/p36/lib/python3.6/site-packages (from nltk) (2021.8.21)\n",
      "Requirement already satisfied: joblib in /home/pi/.local/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /home/pi/anaconda3/envs/p36/lib/python3.6/site-packages (from nltk) (4.61.1)\n",
      "Requirement already satisfied: click in /home/pi/anaconda3/envs/p36/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're going to need NLTK 3. The easiest method to installing the NLTK module is going to be with pip.\n",
    "\n",
    "!pip install nltk\n",
    "\n",
    "# we need to install some of the components for NLTK.\n",
    "import nltk\n",
    "nltk.download()\n",
    "\n",
    "#all (for download everything) --> That will download everything for you headlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c9e7ee-a1d0-44ac-aeae-cc1d6b9731de",
   "metadata": {},
   "source": [
    "## 01. Tokenizing Words and Sentences with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e904cab7-7e3f-4281-98ef-e496ea0b2c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      " ['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"] \n",
      "\n",
      "Tokenized by words:\n",
      " ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "EXAMPLE_TEXT = \"\"\"Hello Mr. Smith, how are you doing today? \n",
    "                The weather is great, and Python is awesome. \n",
    "                The sky is pinkish-blue. You shouldn't eat cardboard.\"\"\"\n",
    "\n",
    "#  This will output the sentences, split up into a list of sentences\n",
    "print('Sentences:\\n',sent_tokenize(EXAMPLE_TEXT),'\\n')\n",
    "\n",
    "# So there, we have created tokens, which are sentences. Let's tokenize by word instead this time\n",
    "print('Tokenized by words:\\n',word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc09e19-805f-423a-b019-235bd4cffd64",
   "metadata": {},
   "source": [
    "* There are a few **things to note** here. \n",
    "    - First, notice that `punctuation is treated as a separate token`. \n",
    "    - Also, notice the separation of the word `\"shouldn't\" into \"should\" and \"n't.\"` \n",
    "    - Finally, notice that `\"pinkish-blue\" is indeed treated like the \"one word\" `it was meant to be turned into. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21919e44-c167-4689-900a-6a5989d4262e",
   "metadata": {},
   "source": [
    "## 02. Stop words with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75865dbb-e3af-4fc3-b09b-16f1167f52b0",
   "metadata": {},
   "source": [
    "```sh\n",
    " We would not want some words like `\"umm\" or \"uhh\"` taking up space in our database, or taking up valuable processing time. \n",
    " As such, we call these words `\"stop words\" because they are useless`, and we wish to do nothing with them. \n",
    " Another version of the term \"stop words\" can be more literal: `Words we stop on`.\n",
    "```\n",
    "![App Screenshot](../images/Stop-word-removal-using-NLTK.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c74c3ff2-f350-40a9-8148-c48a3f893dbb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Here is the list:\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188e762-9cd3-436b-ac46-2fd3668a0094",
   "metadata": {},
   "source": [
    "####  remove the stop words from your text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6c99ef5-5431-45c1-aaf8-acc4ecf2bf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized by words:\n",
      " ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "\n",
      "Filtered sentence:\n",
      " ['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "print('Tokenized by words:\\n',word_tokens)\n",
    "print('\\nFiltered sentence:\\n',filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5092444e-f7ec-492b-a951-5dffbd186912",
   "metadata": {},
   "source": [
    "## 03. Stemming words with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37aa29-d7c5-412f-a63e-76bfefaecb1e",
   "metadata": {},
   "source": [
    " A stemming algorithm reduces the words \n",
    " - `chocolates`, `chocolatey`, `choco` to the root word, `chocolate` \n",
    " and \n",
    " - `retrieval`, `retrieved`, `retrieves` reduce to the stem `retrieve`.\n",
    " \n",
    " One of the most popular stemming algorithms is the `Porter stemmer`, which has been around since 1979."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf65b20-fa82-47ad-88e6-d9ae08de2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these modules\n",
    "from nltk.stem import PorterStemmer\n",
    "  \n",
    "ps = PorterStemmer()\n",
    " \n",
    "# choose some words to be stemmed\n",
    "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
    "\n",
    "# stemming words\n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec1ee0a-2e98-4ebc-90cd-77ef7e92cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming words from sentences\n",
    "sentence = \"Programmers program with programming languages\"\n",
    "words = word_tokenize(sentence)\n",
    "  \n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36",
   "language": "python",
   "name": "p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

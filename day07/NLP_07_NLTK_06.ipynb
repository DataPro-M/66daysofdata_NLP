{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5bb429-4f99-474c-800b-489d0c79165c",
   "metadata": {},
   "source": [
    "# day 6 of #66daysofdata_NLP\n",
    "## NLTK: part 5\n",
    "## Creating a module for Sentiment Analysis with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5835039-8eb4-49c6-a9bc-0fbd9bdb7d78",
   "metadata": {},
   "source": [
    "* ref: \n",
    "    - [https://pythonprogramming.net](https://pythonprogramming.net)\n",
    "    - [https://kaggle.com](https://www.kaggle.com/alvations/sklearn-nltk-voteclassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b4e4e-e2f4-4d1f-99ea-42d36d6584b6",
   "metadata": {},
   "source": [
    "Warning!  This process will take a while.. You may want to just go run some errands. It took me about `30-40 minutes` to run it in full, and I am running an i7 3930k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df2fb16-0f2c-48ea-a7a4-bc86b25d12ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step One: Import nltk and download necessary packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "import nltk\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from scipy import stats as s\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afef1b35-56a4-447a-81c4-beb14c62999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pathes():\n",
    "    path_to_pickle_doc           = 'pickled_algos/documents.pickle'\n",
    "    path_to_pickle_word_features = 'pickled_algos/word_features5k.pickle'\n",
    "p_path = pathes\n",
    "\n",
    "# load the new movie review data\n",
    "with open(\"../datasets/positive.txt\",\"r\", encoding='utf8', errors='ignore') as pos:\n",
    "    short_pos = pos.readlines()\n",
    "with open(\"../datasets/negative.txt\",\"r\", encoding='utf8', errors='ignore') as neg:\n",
    "    short_neg = neg.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08cf8a62-7a3c-4631-851c-34fa792899bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing set (see prev. tutorial)\n",
    "def find_features(document, word_features):\n",
    "        words = word_tokenize(document)\n",
    "        features = {}\n",
    "        for w in word_features:\n",
    "            features[w] = (w in words)\n",
    "        return features\n",
    "    \n",
    "def movie_reviews_to_features(p_path, verbose=0):\n",
    "# move this up here\n",
    "    all_words = []\n",
    "    documents = []\n",
    "\n",
    "    #  j is adject, r is adverb, and v is verb\n",
    "    #allowed_word_types = [\"J\",\"R\",\"V\"]\n",
    "    allowed_word_types = [\"J\"]\n",
    "\n",
    "    def create_document(all_words, documents, reviews, label):\n",
    "        for p in reviews:\n",
    "            documents.append( (p, label) )\n",
    "            words = word_tokenize(p)\n",
    "            pos = nltk.pos_tag(words)\n",
    "            for w in pos:\n",
    "                if w[1][0] in allowed_word_types:\n",
    "                    all_words.append(w[0].lower())\n",
    "        return all_words, documents\n",
    "    all_words, documents = create_document(all_words, documents, short_pos, 'pos')\n",
    "    all_words, documents = create_document(all_words, documents, short_neg, 'neg') \n",
    "    \n",
    "    if verbose ==1:\n",
    "        rev_label = documents[0][1].upper()\n",
    "        rev       = documents[0][0]\n",
    "        print(\"A sample '{}' review : \\n{}\\n\".format(rev_label, rev))\n",
    "    \n",
    "    # save documents\n",
    "    with open(p_path.path_to_pickle_doc, 'wb') as f:\n",
    "        pickle.dump(documents, f)   \n",
    "\n",
    "\n",
    "    # most common words and their counts\n",
    "    all_words = nltk.FreqDist(all_words)\n",
    "    if verbose ==1:\n",
    "        print('The 3 most common words and their counts: \\n{}\\n'.format(all_words.most_common(3)))\n",
    "    # word_features: contains the top 5,000 most common words.\n",
    "    word_features = list(all_words.keys())[:5000]\n",
    "    \n",
    "    # save the word_features\n",
    "    with open(p_path.path_to_pickle_word_features, 'wb') as f:\n",
    "        pickle.dump(word_features, f)\n",
    "\n",
    "    \n",
    "\n",
    "    featuresets = [(find_features(rev, word_features), category) for (rev, category) in documents]\n",
    "\n",
    "    random.shuffle(featuresets)\n",
    "    if verbose ==1:\n",
    "        print(f\"len of the featuresets is: {len(featuresets)}\\n\")\n",
    "\n",
    "    if verbose ==1:\n",
    "        print(\"An Example of first 5 words of a sample featureset of a {} review:\\n{}, label --> {} \\n('True' means the word is in top 5,000 most common words)\".\n",
    "              format(featuresets[0][1],{k: featuresets[0][0][k] for k in list(featuresets[0][0])[:5]}, featuresets[0][1]))\n",
    "    return featuresets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b561e6a1-321a-45d9-8c51-5a0abeed642f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample 'POS' review : \n",
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "\n",
      "The 3 most common words and their counts: \n",
      "[('good', 369), ('more', 331), ('little', 265)]\n",
      "\n",
      "len of the featuresets is: 10662\n",
      "\n",
      "An Example of first 5 words of a sample featureset of a neg review:\n",
      "{'21st': False, 'new': False, 'conan': False, 'greater': False, 'jean-claud': False}, label --> neg \n",
      "('True' means the word is in top 5,000 most common words)\n"
     ]
    }
   ],
   "source": [
    "featuresets = movie_reviews_to_features(p_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "363492b8-a358-47bf-a266-01ddef721c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split\n",
    "testing_set = featuresets[10000:]\n",
    "training_set = featuresets[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155be3a3-6c77-4e5f-b95f-fbbc8894aa07",
   "metadata": {},
   "source": [
    "## Sklearn + NLTK VoteClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ded6056-62fc-4f9f-97ce-93d47e60a0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  'mode' will be our method for choosing the most popular vote\n",
    "from statistics import mode \n",
    "\n",
    "#our classifier class:\n",
    "# we want our new classifier to act like a typical NLTK classifier, \n",
    "# we can just be sure to inherit from the NLTK classifier class. \n",
    "class VoteClassifier:\n",
    "    \n",
    "    def __init__(self, *classifiers_objs):        \n",
    "        #we're assigning the list of classifiers that are passed to our class to self._classifiers.\n",
    "        self.classifiers_objs = classifiers_objs\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        self._classifiers = {}\n",
    "        for clf_obj in self.classifiers_objs:\n",
    "            # NaiveBayesClassifier is part of nltk\n",
    "            if hasattr(clf_obj, '__name__') and clf_obj.__name__ == 'NaiveBayesClassifier':\n",
    "                clf_name = 'NaiveBayesClassifier'\n",
    "                print('Training', clf_name +'\\t'+ str(clf_obj))\n",
    "                clf_obj = nltk.NaiveBayesClassifier.train(training_set)\n",
    "            else:\n",
    "                clf_name = str(clf_obj).split('(')[1]\n",
    "                print('Training', clf_name +'\\t'+ str(clf_obj))\n",
    "                clf_obj.train(training_set)\n",
    "            self._classifiers[clf_name] = clf_obj\n",
    "\n",
    "    def evaluate(self, testing_set):\n",
    "        documents, labels = zip(*testing_set)\n",
    "        predictions = self.classify_documents(documents)\n",
    "        correct = [y == y_hat for y, y_hat in zip(labels, predictions)]\n",
    "        if correct:\n",
    "            return sum(correct) / len(correct)\n",
    "        else:\n",
    "            return 0,0\n",
    "\n",
    "    def classify_documents(self, documents):\n",
    "        return [self.classify_many(doc) for doc in documents]\n",
    "\n",
    "    def classify_many(self, features):\n",
    "        votes = []\n",
    "        for clf_name, clf  in self._classifiers.items():\n",
    "            v = clf.classify(features)\n",
    "            votes.append(v)\n",
    "        #print(votes, mode(votes))\n",
    "        return s.mode(votes)[0][0]\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for clf_name, clf  in self._classifiers.items():\n",
    "            v = clf.classify(features)\n",
    "            votes.append(v)\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d3728e-4bbf-41c3-81fe-6caabf0aafd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NaiveBayesClassifier\t<class 'nltk.classify.naivebayes.NaiveBayesClassifier'>\n",
      "Training MultinomialNB\t<SklearnClassifier(MultinomialNB())>\n",
      "Training BernoulliNB\t<SklearnClassifier(BernoulliNB())>\n",
      "Training LogisticRegression\t<SklearnClassifier(LogisticRegression())>\n",
      "Training SGDClassifier\t<SklearnClassifier(SGDClassifier())>\n",
      "Training LinearSVC\t<SklearnClassifier(LinearSVC())>\n"
     ]
    }
   ],
   "source": [
    "# initiate the VoteClassifier\n",
    "voted_classifier = VoteClassifier(nltk.NaiveBayesClassifier,\n",
    "                                  SklearnClassifier(MultinomialNB()), \n",
    "                                  SklearnClassifier(BernoulliNB()), \n",
    "                                  SklearnClassifier(LogisticRegression()),\n",
    "                                  SklearnClassifier(SGDClassifier()),\n",
    "                                  SklearnClassifier(LinearSVC()),\n",
    "                                  #SklearnClassifier(NuSVC())\n",
    "                                 )\n",
    "\n",
    "# train the VoteClassifier\n",
    "voted_classifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2bcacb0-8136-416c-8e4f-6dc705e1ae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "-------------------------\n",
      "NaiveBayesClassifier \t 74.47129909365559\n",
      "MultinomialNB \t 73.71601208459214\n",
      "BernoulliNB \t 74.01812688821752\n",
      "LogisticRegression \t 71.90332326283988\n",
      "SGDClassifier \t 71.6012084592145\n",
      "LinearSVC \t 71.29909365558912\n",
      "-------------------------\n",
      "VotedClassifier \t 73.86706948640483\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:\\n-------------------------')\n",
    "\n",
    "for clf_name, clf in voted_classifier._classifiers.items():\n",
    "    print(clf_name, '\\t', nltk.classify.accuracy(clf, testing_set)*100)\n",
    "print('-------------------------')\n",
    "print('VotedClassifier', '\\t', voted_classifier.evaluate(testing_set)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b426bda5-7cb1-43c6-a617-c328d4d3f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word features\n",
    "with open(p_path.path_to_pickle_word_features,\"rb\") as vf:\n",
    "    word_features = pickle.load(vf)\n",
    "\n",
    "def sentiment(text):\n",
    "    feats = find_features(text, word_features)\n",
    "    return voted_classifier.classify_many(feats), voted_classifier.confidence(feats)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd5770d0-8e38-4708-9063-97d3642c0449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction result for 'sample_review_01' is --> pos ,Confidence --> 100.0%\n",
      "Prediction result for 'sample_review_02' is --> neg ,Confidence --> 100.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_review_01 = \"This movie was awesome! The acting was great, plot was wonderful, and there were pythons...so good!\"\n",
    "sample_review_02 = \"This movie was utter junk. There were absolutely 0 pythons. I don't see what the point was at all. Horrible movie, 0/10\"\n",
    "\n",
    "print(\"Prediction result for 'sample_review_01' is --> {} ,Confidence --> {}%\".format(sentiment(sample_review_01)[0], sentiment(sample_review_01)[1]))\n",
    "print(\"Prediction result for 'sample_review_02' is --> {} ,Confidence --> {}%\".format(sentiment(sample_review_02)[0], sentiment(sample_review_02)[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36",
   "language": "python",
   "name": "p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

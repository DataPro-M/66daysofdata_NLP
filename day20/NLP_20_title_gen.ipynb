{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5bb429-4f99-474c-800b-489d0c79165c",
   "metadata": {},
   "source": [
    "# day 9 of #66daysofdata_NLP\n",
    "## Machine Learning Model for `Title Generation`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5835039-8eb4-49c6-a9bc-0fbd9bdb7d78",
   "metadata": {},
   "source": [
    "* ref: \n",
    "    - [analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2021/09/building-a-machine-learning-model-for-title-generation/)\n",
    "* data set:\n",
    "    - [Trending YouTube Video Statistics](https://www.kaggle.com/datasnaek/youtube-new) \n",
    "    - Context:\n",
    "        - This dataset is a daily record of the top trending YouTube videos.\n",
    "        - This dataset includes several months (and counting) of data on daily trending YouTube videos. Data is included for the US, GB, DE, CA, and FR regions (USA, Great Britain, Germany, Canada, and France, respectively), with up to 200 listed trending videos per day.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b4e4e-e2f4-4d1f-99ea-42d36d6584b6",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a028462-40f0-41f9-892a-d82dd80c4560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.utils as ku\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)\n",
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a892fce-2cd7-4eab-9604-06c8c2c3c8d2",
   "metadata": {},
   "source": [
    "###  Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08cf8a62-7a3c-4631-851c-34fa792899bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the datasets \n",
    "df1 = pd.read_csv('../datasets/youtube/USvideos.csv')\n",
    "df2 = pd.read_csv('../datasets/youtube/CAvideos.csv')\n",
    "df3 = pd.read_csv('../datasets/youtube/GBvideos.csv')\n",
    "\n",
    "#load the datasets containing the category names\n",
    "data1 = json.load(open('../datasets/youtube/US_category_id.json'))\n",
    "data2 = json.load(open('../datasets/youtube/CA_category_id.json'))\n",
    "data3 = json.load(open('../datasets/youtube/GB_category_id.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8afe13-cf17-4a38-81e8-43f011bb639e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2kyS6SvSYSE</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-13T17:13:01.000Z</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>748374</td>\n",
       "      <td>57527</td>\n",
       "      <td>2966</td>\n",
       "      <td>15954</td>\n",
       "      <td>https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ZAPwfrtAFY</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>The Trump Presidency: Last Week Tonight with J...</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T07:30:00.000Z</td>\n",
       "      <td>last week tonight trump presidency|\"last week ...</td>\n",
       "      <td>2418783</td>\n",
       "      <td>97185</td>\n",
       "      <td>6146</td>\n",
       "      <td>12703</td>\n",
       "      <td>https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>One year after the presidential election, John...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date  \\\n",
       "0  2kyS6SvSYSE      17.14.11   \n",
       "1  1ZAPwfrtAFY      17.14.11   \n",
       "\n",
       "                                               title    channel_title  \\\n",
       "0                 WE WANT TO TALK ABOUT OUR MARRIAGE     CaseyNeistat   \n",
       "1  The Trump Presidency: Last Week Tonight with J...  LastWeekTonight   \n",
       "\n",
       "   category_id              publish_time  \\\n",
       "0           22  2017-11-13T17:13:01.000Z   \n",
       "1           24  2017-11-13T07:30:00.000Z   \n",
       "\n",
       "                                                tags    views  likes  \\\n",
       "0                                    SHANtell martin   748374  57527   \n",
       "1  last week tonight trump presidency|\"last week ...  2418783  97185   \n",
       "\n",
       "   dislikes  comment_count                                  thumbnail_link  \\\n",
       "0      2966          15954  https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg   \n",
       "1      6146          12703  https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg   \n",
       "\n",
       "   comments_disabled  ratings_disabled  video_error_or_removed  \\\n",
       "0              False             False                   False   \n",
       "1              False             False                   False   \n",
       "\n",
       "                                         description  \n",
       "0  SHANTELL'S CHANNEL - https://www.youtube.com/s...  \n",
       "1  One year after the presidential election, John...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kind</th>\n",
       "      <th>etag</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>youtube#videoCategoryListResponse</td>\n",
       "      <td>\"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJv...</td>\n",
       "      <td>{'kind': 'youtube#videoCategory', 'etag': '\"m2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>youtube#videoCategoryListResponse</td>\n",
       "      <td>\"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJv...</td>\n",
       "      <td>{'kind': 'youtube#videoCategory', 'etag': '\"m2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                kind  \\\n",
       "0  youtube#videoCategoryListResponse   \n",
       "1  youtube#videoCategoryListResponse   \n",
       "\n",
       "                                                etag  \\\n",
       "0  \"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJv...   \n",
       "1  \"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJv...   \n",
       "\n",
       "                                               items  \n",
       "0  {'kind': 'youtube#videoCategory', 'etag': '\"m2...  \n",
       "1  {'kind': 'youtube#videoCategory', 'etag': '\"m2...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df1.head(2))\n",
    "\n",
    "display(pd.DataFrame(data1).head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d9e5f-2d4e-4094-b74a-98614a11eff3",
   "metadata": {},
   "source": [
    "### clean up and process the data:\n",
    "\n",
    "* Now we must process our data in order to utilize it to train our machine learning model for the purpose of topic generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c6de00-9b2a-42f8-924d-70c232698a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_extractor(data):\n",
    "    i_d = [data['items'][i]['id'] for i in range(len(data['items']))]\n",
    "    title = [data['items'][i]['snippet'][\"title\"] for i in range(len(data['items']))]\n",
    "    i_d = list(map(int, i_d))\n",
    "    category = zip(i_d, title)\n",
    "    category = dict(category)\n",
    "    return category\n",
    "\n",
    "#create a new category column by mapping the category names to their id\n",
    "df1['category_title'] = df1['category_id'].map(category_extractor(data1))\n",
    "df2['category_title'] = df2['category_id'].map(category_extractor(data2))\n",
    "df3['category_title'] = df3['category_id'].map(category_extractor(data3))\n",
    "\n",
    "#join the dataframes\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "#drop rows based on duplicate videos\n",
    "df = df.drop_duplicates('video_id')\n",
    "\n",
    "#collect only titles of entertainment videos\n",
    "#feel free to use any category of video that you want\n",
    "entertainment = df[df['category_title'] == 'Entertainment']['title']\n",
    "entertainment = entertainment.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c021a29-7989-40aa-b07b-4fcd0e8f9301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_title_Entertainment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Trump Presidency: Last Week Tonight with J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nickelback Lyrics: Real or Fake?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I Dare You: GOING BALD!?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Roy Moore &amp; Jeff Sessions Cold Open - SNL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(SPOILERS) 'Shiva Saves the Day' Talked About ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9725</th>\n",
       "      <td>[SHINee - Good Evening] Comeback Stage | M COU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9726</th>\n",
       "      <td>JUSTICE LEAGUE Is Better Than Infinity Wars | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9727</th>\n",
       "      <td>Diddy &amp; King Combs on The Four, Rap Beef, NFL ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9728</th>\n",
       "      <td>Hilary Duff Is Having a Baby Girl and Her Son ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9729</th>\n",
       "      <td>GLOW: Season 2 | Main Trailer [HD] | Netflix</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9730 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           category_title_Entertainment\n",
       "0     The Trump Presidency: Last Week Tonight with J...\n",
       "1                      Nickelback Lyrics: Real or Fake?\n",
       "2                              I Dare You: GOING BALD!?\n",
       "3             Roy Moore & Jeff Sessions Cold Open - SNL\n",
       "4     (SPOILERS) 'Shiva Saves the Day' Talked About ...\n",
       "...                                                 ...\n",
       "9725  [SHINee - Good Evening] Comeback Stage | M COU...\n",
       "9726  JUSTICE LEAGUE Is Better Than Infinity Wars | ...\n",
       "9727  Diddy & King Combs on The Four, Rap Beef, NFL ...\n",
       "9728  Hilary Duff Is Having a Baby Girl and Her Son ...\n",
       "9729       GLOW: Season 2 | Main Trailer [HD] | Netflix\n",
       "\n",
       "[9730 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame( {'category_title_Entertainment':entertainment}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0965a2e-7002-4079-b922-7b46837c7fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuations and convert text to lowercase\n",
    "def clean_text(text):\n",
    "    text = ''.join(e for e in text if e not in string.punctuation).lower()\n",
    "    \n",
    "    text = text.encode('utf8').decode('ascii', 'ignore')\n",
    "    return text\n",
    "\n",
    "corpus = [clean_text(e) for e in entertainment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c02af7c-d63f-4379-a361-d8d2b0948979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence 0: 'the trump presidency last week tonight with john oliver hbo'\n",
      "Sample sentence 1: 'nickelback lyrics real or fake'\n",
      "Sample sentence 2: 'i dare you going bald'\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(corpus):\n",
    "    print(f\"Sample sentence {i}: '{sentence}'\") if i <3 else False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2af4bd-c833-4abb-9fca-0f444e2354c9",
   "metadata": {},
   "source": [
    "### Generating sequences using  Tokenizer  API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff49f709-2fc4-4470-9a92-e74fec021580",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    #get tokens\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    #convert to sequence of tokens\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    return input_sequences, total_words\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b34d3b2f-1fec-486b-bca4-0b1964a393f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of inp_sequences is '70438' and the corpus has '13915' words\n"
     ]
    }
   ],
   "source": [
    "print(f\"len of inp_sequences is '{len(inp_sequences)}' and the corpus has '{total_words}' words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7943517-059c-4dbe-b53f-46ef3bc1a194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample corpus sentence:\n",
      "the trump presidency last week tonight with john oliver hboits sequence_of_tokens: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 87],\n",
       " [1, 87, 3083],\n",
       " [1, 87, 3083, 70],\n",
       " [1, 87, 3083, 70, 353],\n",
       " [1, 87, 3083, 70, 353, 1179],\n",
       " [1, 87, 3083, 70, 353, 1179, 11],\n",
       " [1, 87, 3083, 70, 353, 1179, 11, 135],\n",
       " [1, 87, 3083, 70, 353, 1179, 11, 135, 991],\n",
       " [1, 87, 3083, 70, 353, 1179, 11, 135, 991, 1432]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How does it work?\n",
    "print(f'sample corpus sentence:\\n{corpus[0]}its sequence_of_tokens: ')\n",
    "get_sequence_of_tokens([corpus[0]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0080b-0477-4439-83cc-592c3565363d",
   "metadata": {},
   "source": [
    "### Padding the sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2c24a79-ac91-4c8f-b6db-58e3775109a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    \n",
    "    # finding the max length of input sequence lists\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    \n",
    "    # The pad_sequences() is a function in the Keras deep learning library that can be used to pad variable-length sequences \n",
    "    input_sequences = np.array(pad_sequences(input_sequences,  maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    # use last item of every input sequence as our moder label for next word prediction task\n",
    "    predictors, label = input_sequences[:,:-1], input_sequences[:, -1]\n",
    "    \n",
    "    # convert labels to categorical data\n",
    "    label = ku.to_categorical(label, num_classes = total_words)\n",
    "    \n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fca178e-9221-4efb-a954-3bd75da7f23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (70438, 26)\n",
      "Output shape: (70438, 13915)\n",
      "max sequence length: 27\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input shape: {predictors.shape}\\nOutput shape: {label.shape}\\nmax sequence length: {max_sequence_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e0064-ef4b-4294-9d24-0628d30f234d",
   "metadata": {},
   "source": [
    "## LSTM Model for Title Generation\n",
    "The LSTM of this model consists of three layers:\n",
    "\n",
    "    Input layer: takes the word order as input\n",
    "    LSTM Layout: Calculate output using LSTM units.\n",
    "    Disposal layer: a regular layer to avoid overheating\n",
    "    Output layer: determines whether the next word may be output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da7b0357-ec66-4f2a-be5f-8a36f44e6200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2202/2202 [==============================] - 42s 19ms/step - loss: 7.9290\n",
      "Epoch 2/20\n",
      "2202/2202 [==============================] - 42s 19ms/step - loss: 7.0978\n",
      "Epoch 3/20\n",
      "2202/2202 [==============================] - 40s 18ms/step - loss: 6.6247\n",
      "Epoch 4/20\n",
      "2202/2202 [==============================] - 44s 20ms/step - loss: 6.2269\n",
      "Epoch 5/20\n",
      "2202/2202 [==============================] - 41s 18ms/step - loss: 5.8632\n",
      "Epoch 6/20\n",
      "2202/2202 [==============================] - 41s 19ms/step - loss: 5.5172\n",
      "Epoch 7/20\n",
      "2202/2202 [==============================] - 45s 20ms/step - loss: 5.2035\n",
      "Epoch 8/20\n",
      "2202/2202 [==============================] - 42s 19ms/step - loss: 4.9172\n",
      "Epoch 9/20\n",
      "2202/2202 [==============================] - 44s 20ms/step - loss: 4.6577\n",
      "Epoch 10/20\n",
      "2202/2202 [==============================] - 42s 19ms/step - loss: 4.4215\n",
      "Epoch 11/20\n",
      "2202/2202 [==============================] - 42s 19ms/step - loss: 4.2031\n",
      "Epoch 12/20\n",
      "2202/2202 [==============================] - 42s 19ms/step - loss: 3.9951\n",
      "Epoch 13/20\n",
      "2202/2202 [==============================] - 41s 19ms/step - loss: 3.8063\n",
      "Epoch 14/20\n",
      "2202/2202 [==============================] - 40s 18ms/step - loss: 3.6318\n",
      "Epoch 15/20\n",
      "2202/2202 [==============================] - 45s 20ms/step - loss: 3.4666\n",
      "Epoch 16/20\n",
      "2202/2202 [==============================] - 45s 20ms/step - loss: 3.3149\n",
      "Epoch 17/20\n",
      "2202/2202 [==============================] - 40s 18ms/step - loss: 3.1736\n",
      "Epoch 18/20\n",
      "2202/2202 [==============================] - 39s 18ms/step - loss: 3.0425\n",
      "Epoch 19/20\n",
      "2202/2202 [==============================] - 41s 19ms/step - loss: 2.9266\n",
      "Epoch 20/20\n",
      "2202/2202 [==============================] - 42s 19ms/step - loss: 2.8104\n",
      "Now that our title generator learning model is ready \n"
     ]
    }
   ],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "\n",
    "    # Add Hidden Layer 1 — LSTM Layer\n",
    "    model.add((LSTM(100)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "# create the model\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "\n",
    "# fit the model\n",
    "model.fit(predictors, label, epochs=20,verbose=1)\n",
    "print('Now that our title generator learning model is ready ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "55183ea5-16b3-4656-a59d-565a2894eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for i in range(next_words -1):\n",
    "        \n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1,  padding='pre')\n",
    "        \n",
    "        if version.parse(tf.__version__) >= version.parse('2.6.0'):\n",
    "            predicted = np.argmax(model.predict(token_list), axis=-1)[0]            \n",
    "        else:\n",
    "            predicted = model.predict_classes(token_list, verbose=0)\n",
    "\n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break       \n",
    "        \n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b85aa33e-b86a-418b-9ab2-faeb6e9a78b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'Drake' ---> Output: 'Drake Talks On His Own'\n",
      "\n",
      "Input: 'united states' ---> Output: 'United States Promo Scene'\n",
      "\n",
      "Input: 'Spider man' ---> Output: 'Spider Man On The Breakfast Club'\n",
      "\n",
      "Input: 'Donald Trump' ---> Output: 'Donald Trump To Missed In The World'\n",
      "\n",
      "Input: 'Sara' ---> Output: 'Sara Williams Is Back'\n",
      "\n",
      "Input: 'Minnesota' ---> Output: 'Minnesota Vs Williams'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key_words = ['Drake', 'united states', 'Spider man', 'Donald Trump', 'Sara', 'Minnesota']\n",
    "title_len = [5, 3, 5, 6, 4, 3]\n",
    "\n",
    "for key_word, t_len in zip(key_words, title_len):\n",
    "    print (f\"Input: '{key_word}' ---> Output: '{generate_text(key_word, t_len, model, max_sequence_len)}'\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36",
   "language": "python",
   "name": "p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
